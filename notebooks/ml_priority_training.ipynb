{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸŽ¯ ML Priority Scoring System - Training & Evaluation\n",
    "\n",
    "This notebook trains and evaluates three ML models (Random Forest, XGBoost, LightGBM) to predict:\n",
    "1. **Modernization Score** - Probability equipment needs modernization\n",
    "2. **OEM Score** - Probability equipment needs total replacement\n",
    "3. **Maintenance Score** - Probability equipment needs lifecycle services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import duckdb\n",
    "\n",
    "from app.services.ml_priority_service import MLPriorityService\n",
    "from app.core.config import settings\n",
    "\n",
    "# Visualization settings\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"âœ… Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Connect to Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to DuckDB\n",
    "db_path = settings.DB_PATH\n",
    "conn = duckdb.connect(str(db_path))\n",
    "\n",
    "# Check available tables\n",
    "tables = conn.execute(\"SHOW TABLES\").df()\n",
    "print(\"Available tables:\")\n",
    "print(tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialize ML Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize service with database connection\n",
    "ml_service = MLPriorityService(db_conn=conn)\n",
    "print(\"âœ… ML Service initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extract & Explore Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract training data\n",
    "df = ml_service.extract_training_data()\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nColumns: {df.columns.tolist()}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize equipment age distribution\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "df['equipment_age'].hist(bins=30, edgecolor='black')\n",
    "plt.xlabel('Equipment Age (years)')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Equipment Age Distribution')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "df['age_category'].value_counts().plot(kind='bar')\n",
    "plt.xlabel('Age Category')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Equipment by Age Category')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "df.groupby('equipment_type_clean')['equipment_age'].mean().sort_values(ascending=False).head(10).plot(kind='barh')\n",
    "plt.xlabel('Average Age (years)')\n",
    "plt.title('Top 10 Equipment Types by Avg Age')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generate Training Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate labels\n",
    "df = ml_service.generate_labels(df)\n",
    "\n",
    "# Visualize label distribution\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for idx, (target, ax) in enumerate(zip(['modernization_target', 'oem_target', 'maintenance_target'], axes)):\n",
    "    df[target].value_counts().plot(kind='bar', ax=ax, color=['lightcoral', 'lightblue'])\n",
    "    ax.set_title(f\"{target.replace('_target', '').title()} Labels\")\n",
    "    ax.set_xlabel('Label')\n",
    "    ax.set_ylabel('Count')\n",
    "    ax.set_xticklabels(['No', 'Yes'], rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train models for all three targets\n",
    "ml_service.train_all_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect performance metrics\n",
    "performance_data = []\n",
    "\n",
    "for target_type in ['modernization', 'oem', 'maintenance']:\n",
    "    for model_name, model_data in ml_service.models[target_type].items():\n",
    "        metrics = model_data['metrics']\n",
    "        performance_data.append({\n",
    "            'Target': target_type.title(),\n",
    "            'Model': metrics['model_name'],\n",
    "            'AUC-ROC': metrics['auc_roc'],\n",
    "            'AUC-PR': metrics['auc_pr'],\n",
    "            'Accuracy': metrics['accuracy']\n",
    "        })\n",
    "\n",
    "perf_df = pd.DataFrame(performance_data)\n",
    "print(\"\\nðŸ“Š Model Performance Summary:\")\n",
    "print(perf_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for idx, metric in enumerate(['AUC-ROC', 'AUC-PR', 'Accuracy']):\n",
    "    pivot = perf_df.pivot(index='Model', columns='Target', values=metric)\n",
    "    pivot.plot(kind='bar', ax=axes[idx], width=0.8)\n",
    "    axes[idx].set_title(f'{metric} Comparison')\n",
    "    axes[idx].set_ylabel(metric)\n",
    "    axes[idx].set_ylim([0, 1])\n",
    "    axes[idx].legend(title='Target Type')\n",
    "    axes[idx].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Generate Priority List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions for all equipment\n",
    "priority_df = ml_service.predict_priorities(df)\n",
    "\n",
    "# Display top priorities\n",
    "print(\"\\nðŸŽ¯ TOP 20 PRIORITY EQUIPMENT FOR MODERNIZATION:\")\n",
    "priority_cols = [\n",
    "    'company_internal', 'equipment_type_clean', 'equipment_age',\n",
    "    'modernization_score', 'oem_score', 'maintenance_score', 'priority_rank'\n",
    "]\n",
    "print(priority_df[priority_cols].head(20).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize score distributions\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for idx, score_col in enumerate(['modernization_score', 'oem_score', 'maintenance_score']):\n",
    "    axes[idx].hist(priority_df[score_col], bins=30, edgecolor='black', alpha=0.7)\n",
    "    axes[idx].axvline(priority_df[score_col].median(), color='red', linestyle='--', label=f'Median: {priority_df[score_col].median():.1f}')\n",
    "    axes[idx].set_xlabel('Score')\n",
    "    axes[idx].set_ylabel('Count')\n",
    "    axes[idx].set_title(f\"{score_col.replace('_', ' ').title()} Distribution\")\n",
    "    axes[idx].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Export Priority List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to CSV\n",
    "output_file = '../data/priority_list.csv'\n",
    "priority_df[priority_cols].to_csv(output_file, index=False)\n",
    "print(f\"âœ… Priority list exported to: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save to DuckDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save priority scores to database\n",
    "conn.execute(\"DROP TABLE IF EXISTS equipment_priorities\")\n",
    "conn.execute(\"CREATE TABLE equipment_priorities AS SELECT * FROM priority_df\")\n",
    "print(\"âœ… Priority scores saved to DuckDB table: equipment_priorities\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
